{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e532fa-f067-4ac8-a54b-b3ea518c4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from langchain_xai import ChatXAI\n",
    "#from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030082ae-bb96-414e-8968-47fd5c542cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = ChatXAI(model='grok-2-latest', \n",
    "              temperature=0.01, \n",
    "              max_tokens=None, \n",
    "              timeout=None, \n",
    "              max_retries=2, \n",
    "              api_key='your api key')\n",
    "\n",
    "#llm = OpenAI(model='gpt-4o') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ea6e13f-b5bb-42ca-98cc-81b4946a3d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India gained independence from British rule on August 15, 1947.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"When did India get Independence?\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4638a50-8c70-4d68-b72e-539ef6a2ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin Tendulkar is a former Indian cricketer widely regarded as one of the greatest cricketers of all time. Born on April 24, 1973, in Mumbai, India, he made his debut for the Indian cricket team at the age of 16 in 1989. Throughout his illustrious career, Tendulkar played 200 Test matches and 463 One Day Internationals (ODIs), scoring 15,921 runs in Tests and 18,426 runs in ODIs, both of which are world records.\n",
      "\n",
      "Known as the \"Master Blaster,\" Tendulkar was renowned for his technique, consistency, and ability to play under pressure. He holds numerous records, including the most runs in international cricket, the most centuries in both Test and ODI cricket, and being the first player to score a double century in an ODI.\n",
      "\n",
      "Tendulkar retired from cricket in 2013, leaving behind a legacy that continues to inspire millions. He was awarded the Bharat Ratna, India's highest civilian honor, in 2014, becoming the first sportsperson to receive this award. Post-retirement, he has been involved in various cricket-related activities and philanthropy.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"Who is Sachin Tendulkar\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e071d4f2-9b4a-4c47-a0a5-b0e92282b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment expressed in your statement can be classified as **mixed** or **neutral** overall. Here's the breakdown:\n",
      "\n",
      "- **Negative sentiment**: \"The movie was very bad\"\n",
      "- **Positive sentiment**: \"Vicky Kaushal's acting was really good\"\n",
      "\n",
      "The mention of the movie being \"very bad\" indicates a negative sentiment towards the film as a whole. However, the positive comment about Vicky Kaushal's acting introduces a positive sentiment specifically towards his performance.\n",
      "\n",
      "When sentiments within a single statement are contradictory, it's often classified as neutral or mixed, as it contains both positive and negative elements.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"The movie was very bad, but Vicky Kaushals acting was really good, can you classify it as positive/negative/neutral sentiment\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77fb7231-7591-4758-b5e6-bbaecc363938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"How are you\" in Spanish is \"¿Cómo estás?\" if you are addressing one person informally, or \"¿Cómo está usted?\" if you are addressing one person formally. If you are addressing a group, you would say \"¿Cómo están?\"\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"What is How are you in Spanish\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b3281-df47-4979-8f87-2ef8bd9bb212",
   "metadata": {},
   "source": [
    "## Implementing RAG for custom data (Research Paper Chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9748d189-6817-4730-bfa4-db0f9820c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9276aed2-315a-4c48-aa31-82489091c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"RAGPaper.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6777210-fc9e-437e-b894-7d2e2b4489fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9192\\2923984562.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings using a free HF model\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba4ce3-4ebd-40db-9781-192241fad061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = ChatXAI(model='grok-2-latest', \n",
    "              temperature=0.01, \n",
    "              max_tokens=None, \n",
    "              timeout=None, \n",
    "              max_retries=2, \n",
    "              api_key='your api key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f1014b6-0c8f-4a3f-97fa-482ff9ad6c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India gained independence from British rule on August 15, 1947.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"When did India get Independence?\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e675adf-d177-4394-896e-eb313041a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow question, rephrase the follow up question to be a standalone question.\n",
    "                                                        Chat History:{chat_history}\n",
    "                                                        Follow up Input: {question}\n",
    "                                                        Standalone question:\"\"\")\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True,\n",
    "                                           verbose=False)\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea366111-5658-4963-aeb8-058620041f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"what is a RAG-sequence model?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2a13638-1737-41d1-a05d-e6428cf170c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RAG-Sequence model is a type of Retrieval-Augmented Generation (RAG) model where the entire output sequence is generated based on a single retrieved document. In this model, the top K documents are retrieved using a retriever, and then the generator produces the output sequence probability for each document. These probabilities are then marginalized to approximate the sequence probability \\( p(y|x) \\). The model treats the retrieved document as a single latent variable and uses a top-K approximation to compute the sequence probability. \n",
      "\n",
      "For decoding, RAG-Sequence does not break into a conventional per-token likelihood, so it uses a method called \"Thorough Decoding.\" This involves running beam search for each document, scoring hypotheses, and then estimating the probability of each hypothesis by running additional forward passes for documents where the hypothesis did not appear in the beam. This process can become computationally intensive for longer output sequences.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edd4f1f3-fd95-4299-b309-0890d5dcfa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training setup details for the models described in the paper are as follows:\n",
      "\n",
      "- **Training Framework**: All RAG models and BART baselines were trained using Fairseq, a sequence modeling toolkit.\n",
      "\n",
      "- **Hardware and Precision**: Training was conducted with mixed precision floating-point arithmetic and distributed across 8 NVIDIA V100 GPUs, each with 32GB of memory.\n",
      "\n",
      "- **Optimization**: The training process involved minimizing the negative marginal log-likelihood of each target using stochastic gradient descent with the Adam optimizer.\n",
      "\n",
      "- **Document Encoder**: The document encoder, referred to as BERTd, was not updated during training. This decision was made to avoid the need for periodic updates to the document index, which is a costly process. Instead, the document encoder and its index were kept fixed.\n",
      "\n",
      "- **Fine-Tuning**: Only the query encoder (BERTq) and the BART generator were fine-tuned during the training process.\n",
      "\n",
      "These details outline the approach taken to train the models effectively while managing computational resources and optimizing performance.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"Can you explain the training setup details explained in this paper?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "108c3b4b-487e-414b-91e9-a7f83ac6fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 references used in the provided context are:\n",
      "\n",
      "1. **Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009.**\n",
      "   - This reference discusses the probabilistic relevance framework, including the BM25 model and its extensions.\n",
      "\n",
      "2. **Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019.**\n",
      "   - This reference explores the release strategies and social impacts associated with language models.\n",
      "\n",
      "3. **Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In Advances in Neural Information Processing Systems 28, pages 2440–2448, 2015.**\n",
      "   - This reference introduces end-to-end memory networks, a type of neural network architecture.\n",
      "\n",
      "4. **Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1870–1879, 2017.**\n",
      "   - This reference discusses a method for answering open-domain questions using Wikipedia as a knowledge source.\n",
      "\n",
      "5. **Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and others. From Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, 2013.**\n",
      "   - This reference focuses on generating answers from question-answer pairs in natural language processing.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What are the top 5 references used in this paper?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa4161-2a9e-4a84-b6ac-0e8a21f36fbc",
   "metadata": {},
   "source": [
    "## AI Tutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a94f345f-3040-48e1-98e5-242eeaa798b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0df65e35-1a03-43df-a508-e537c213f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"Power BI Ebook.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b07a8c69-759b-4202-a432-85193d9c7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9192\\2923984562.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings using a free HF model\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ff56d-2be1-4584-be12-09dd196aca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = ChatXAI(model='grok-2-latest', \n",
    "              temperature=0.01, \n",
    "              max_tokens=None, \n",
    "              timeout=None, \n",
    "              max_retries=2, \n",
    "              api_key='your api key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35a56dd7-2ce0-4202-aae3-2739c70844c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Behave as a AI tutor, and respond the queries from the document trained, Given the following conversation and a follow question, rephrase the follow up question to be a standalone question.\n",
    "                                                        Chat History:{chat_history}\n",
    "                                                        Follow up Input: {question}\n",
    "                                                        Standalone question:\"\"\")\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True,\n",
    "                                           verbose=False)\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79e4be86-6c23-4dce-a496-82a432cb5cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement Row-Level Security (RLS) in Power BI, follow these steps:\n",
      "\n",
      "1. **Define your roles**: Define the roles for your users, such as sales reps, managers, or regional directors.\n",
      "\n",
      "2. **Define your rules**: Define the rules that control what data each role can see. For example, you might limit a sales rep to only seeing data for their territory, or a manager to only seeing data for their team.\n",
      "\n",
      "3. **Implement your rules**: Implement your RLS rules by creating DAX expressions that define the rules for each role. These expressions will be used to filter the data that each user sees.\n",
      "\n",
      "4. **Test your rules**: Test your RLS rules by previewing the report or dashboard as each role. Ensure that each role only sees the data that they are supposed to see.\n",
      "\n",
      "Here's an example of how to implement RLS in Power BI:\n",
      "\n",
      "1. **Create a new role**: Click on the Manage Roles option in the Modeling tab and create a new role, such as \"Sales Rep\".\n",
      "\n",
      "2. **Define your rules**: Create a DAX expression that filters the data based on the Sales Rep's territory. For example: `=Sales[Territory]=USERPRINCIPALNAME()`.\n",
      "\n",
      "3. **Implement your rules**: Apply the Sales Rep role to the report or dashboard and ensure that the Sales Rep only sees data related to their territory.\n",
      "\n",
      "4. **Test your rules**: Preview the report or dashboard as the Sales Rep and ensure that they only see data related to their territory.\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"What are the steps to implement RLS?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de30b3f-dd4b-4d36-a3e0-e11225621faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
