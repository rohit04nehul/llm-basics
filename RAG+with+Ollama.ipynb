{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e532fa-f067-4ac8-a54b-b3ea518c4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "030082ae-bb96-414e-8968-47fd5c542cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = Ollama(model='tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ea6e13f-b5bb-42ca-98cc-81b4946a3d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10856\\3598444284.py:3: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India got Independence on August 15, 1947. It was achieved through the resolution of the Indian National Congress and the British Raj to give complete independence to India from British rule, which ended their 208-year long rule over the country.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"When did India get Independence?\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4638a50-8c70-4d68-b72e-539ef6a2ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sachin Tendulkar is an Indian former cricketer who played as a left-handed batsman for Karnataka and India. He was known for his aggressive style of play and scoring runs at a rapid pace. He made his debut in the Ranji Trophy in 1972, and went on to represent India in the following years. During his international career, he played 48 Test matches, scoring 2636 runs with an average of 45.10. He was also a member of the Indian team that won the 1983 World Cup. In terms of domestic cricket, Tendulkar was known for his explosive innings and was a key player in Karnataka's historic win against Mumbai Indians in the IPL in 2014. He retired from international cricket in 1987 and later moved into coaching and management roles with various Indian cricket teams, including BCCI and T20 franchises.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"Who is Sachin Tendulkar\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b3281-df47-4979-8f87-2ef8bd9bb212",
   "metadata": {},
   "source": [
    "## Implementing RAG for custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9748d189-6817-4730-bfa4-db0f9820c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9276aed2-315a-4c48-aa31-82489091c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDFLoader(\"RAGPaper.pdf\")\n",
    "documents = pdf_reader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6777210-fc9e-437e-b894-7d2e2b4489fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10856\\2923984562.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10856\\2923984562.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings using a free HF model\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(documents=chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62ba4ce3-4ebd-40db-9781-192241fad061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = Ollama(model='tinyllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f1014b6-0c8f-4a3f-97fa-482ff9ad6c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India got independence on August 15, 1947. It was declared as a republic by the British Empire's Governor General, Lord Mountbatten. The ceremony of India's Independence Day was held in Delhi on August 15, 1947, at Rajghat.\n"
     ]
    }
   ],
   "source": [
    "# Generate answers to a question\n",
    "question = \"When did India get Independence?\"\n",
    "response = llm.predict(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e675adf-d177-4394-896e-eb313041a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow question, rephrase the follow up question to be a standalone question.\n",
    "                                                        Chat History:{chat_history}\n",
    "                                                        Follow up Input: {question}\n",
    "                                                        Standalone question:\"\"\")\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt=CONDENSE_QUESTION_PROMPT, return_source_documents=True,\n",
    "                                           verbose=False)\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea366111-5658-4963-aeb8-058620041f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"what is a RAG-sequence model?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2a13638-1737-41d1-a05d-e6428cf170c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A RAG-Sequence model is a type of generative model that allows for multiple output tokens to be produced by a single document, using a topic-word sequence as the input. The sequence is generated by iteratively selecting and combining words based on their topic probabilities, such that each word appears in a specific order and contributes to the overall message or theme being conveyed in the text.\n",
      "\n",
      "The key concept of RAG-Sequence is the use of the topic-word sequence as the input to generate multiple output tokens at once, rather than using a single beam search for all possible outputs. This allows for efficient decoding by running multiple forward passes for each document and combining probabilities across beams in a thresholded manner.\n",
      "\n",
      "The RAG-Sequence model is particularly useful when producing short or medium-length text sequences that require many output tokens to convey their meaning effectively, such as news articles or social media posts.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edd4f1f3-fd95-4299-b309-0890d5dcfa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query \"Who are the authors of this paper?\" can be answered using a more specific and factually accurate response provided by the RAG model. Here is an example:\n",
      "\n",
      "Task Input: Middl-ear, 3rd-person singular(s), possessive(s)\n",
      "Question: Which authors were mentioned in the paper?\n",
      "Helpful Answer: The answer provided by the RAG model is:\n",
      "\n",
      "Model Generated Response: \"Authors include\n"
     ]
    }
   ],
   "source": [
    "chat_history=[]\n",
    "query=\"\"\"who are the authors of this paper?\"\"\"\n",
    "result = qa({\"question\":query, \"chat_history\":chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c3b4b-487e-414b-91e9-a7f83ac6fa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
